{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12937e4",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "One of the methodologies to efficiently index data for future usage, is tagging them before saving them in the dataset. For example, on Reddit, posts will be tagged by the subreddit. Assuming a scenario that the Reddit server would not be able to process data due to any reason (say hardware limitation or unexpected behavior after an update), we need a classifier to process data and tag each post. The goal of this project is to provide the possibility of tagging Reddit posts using NLP and machine learning. For the use case, posts from two subreddits are extracted: \n",
    "\n",
    "Samsunggalaxy and iPhone\n",
    "\n",
    "Therefore, we intend to make a binary classifier in order to tag each Reddit post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ead5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d20fb",
   "metadata": {},
   "source": [
    "### Defining scraper using Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390e6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_posts(target_url, count, bot_header, sleep):\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    for i in range(count):\n",
    "        if after==None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after' : after}\n",
    "        \n",
    "        res = requests.get(url=target_url, params=params, headers=bot_header)\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            after = the_json['data']['after']\n",
    "        \n",
    "        else:\n",
    "            # this confirms that an error happens\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        \n",
    "        time.sleep(sleep)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def prepare_df(raw_data, cols):\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    for col in cols:\n",
    "        df[col] = df['data'].map(lambda x: x[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606d0ff",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c124b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\n",
    "header = {'User-agent': 'Bleep blorp bot 0.1'}\n",
    "count = 200\n",
    "sleep_value = 0.02\n",
    "selected_columns = [\"subreddit\", \"title\", \"name\", \"selftext\", \"domain\", \"upvote_ratio\",\n",
    "                    \"score\", \"subreddit_id\", \"is_robot_indexable\", \"author\",\n",
    "                    \"num_comments\", \"send_replies\", \"is_video\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5bcb4",
   "metadata": {},
   "source": [
    "### Extracting Samsung Galaxy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f7cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/samsunggalaxy/.json'\n",
    "# Step one: data extraction from reddit API\n",
    "raw_scraped_galaxy = extract_reddit_posts(target_url=url, count=count, bot_header=header, sleep=sleep_value)### Extracting Samsung Galaxy Data\n",
    "# Step two: conversion raw extracted data into pandas dataframe and filtering columns\n",
    "galaxy_df = prepare_df(raw_data=raw_scraped_galaxy, cols=selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9603d33",
   "metadata": {},
   "source": [
    "### Extracting Iphone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22820d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/r/iphone/.json'\n",
    "raw_scraped_iphone = extract_reddit_posts(target_url=url, count=count, bot_header=header, sleep=sleep_value)\n",
    "iphone_df = prepare_df(raw_data=raw_scraped_iphone, cols=selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa6f0c",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b580e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory = \"./data/\"\n",
    "galaxy_df.to_csv(dataDirectory + \"galaxy.csv\", encoding='utf-8')\n",
    "iphone_df.to_csv(dataDirectory + \"iphone.csv\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
